{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Effectiveness of Bank Marketing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Author:` Rasika Guru (rguru@usc.edu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This mini-ML project aims at predicting if a person will subscribe to a bank's term deposit or not based on previous outcomes of bank marketing campaigns of a Portuguese banking institution. For more information about the data see: https://archive.ics.uci.edu/ml/datasets/bank+marketing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing packages that we'll use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import sklearn.preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Reading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   age          job  marital          education default  housing     loan  \\\n",
      "0   30  blue-collar  married           basic.9y      no      yes       no   \n",
      "1   39     services   single        high.school      no       no       no   \n",
      "2   25     services  married        high.school      no      yes       no   \n",
      "3   38     services  married           basic.9y      no  unknown  unknown   \n",
      "4   47       admin.  married  university.degree      no      yes       no   \n",
      "\n",
      "     contact month day_of_week ...  campaign  pdays  previous     poutcome  \\\n",
      "0   cellular   may         fri ...         2    999         0  nonexistent   \n",
      "1  telephone   may         fri ...         4    999         0  nonexistent   \n",
      "2  telephone   jun         wed ...         1    999         0  nonexistent   \n",
      "3  telephone   jun         fri ...         3    999         0  nonexistent   \n",
      "4   cellular   nov         mon ...         1    999         0  nonexistent   \n",
      "\n",
      "  emp.var.rate  cons.price.idx  cons.conf.idx  euribor3m  nr.employed   y  \n",
      "0         -1.8          92.893          -46.2      1.313       5099.1  no  \n",
      "1          1.1          93.994          -36.4      4.855       5191.0  no  \n",
      "2          1.4          94.465          -41.8      4.962       5228.1  no  \n",
      "3          1.4          94.465          -41.8      4.959       5228.1  no  \n",
      "4         -0.1          93.200          -42.0      4.191       5195.8  no  \n",
      "\n",
      "[5 rows x 21 columns]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('Data/bank-additional.csv', delimiter=';')\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with Categorical features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is observed that many features are categorical. For scikit-learn classifiers to work with this, we one-hot encode these features to get dummy variables. The following function does this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def hot_encoder(df, column_name):\n",
    "    column = df[column_name].tolist() \n",
    "    lab_enc = sklearn.preprocessing.LabelEncoder()\n",
    "    lab_enc.fit(column)\n",
    "    enc_column = lab_enc.transform(column)\n",
    "    enc_column = np.reshape(enc_column, (len(enc_column), 1)) \n",
    "    enc = sklearn.preprocessing.OneHotEncoder()\n",
    "    enc.fit(enc_column)\n",
    "    new_column = enc.transform(enc_column).toarray()\n",
    "    column_titles = []\n",
    "    for i in range(len(new_column[0])):\n",
    "        this_column_name = column_name+\"_\"+str(i)\n",
    "        df[this_column_name] = new_column[:,i]\n",
    "    df.drop(column_name, axis=1, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "categorical_features = ['job', 'marital', 'education', 'default', 'housing', 'loan', 'contact', 'month', 'day_of_week', 'poutcome']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All categorical features are one hot encoded using the function written above. Also note that the feature 'duration' is removed as the duration of call is known only after the phone call's end, which should not be used for prediction as the outcome is also known when the call is over. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4119, 63)\n"
     ]
    }
   ],
   "source": [
    "for feature in categorical_features:\n",
    "    df = hot_encoder(df, feature)\n",
    "df.drop('duration', axis=1, inplace=True)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have 62 features + the response variable y and 4119 samples. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting Data into train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4119, 62)\n",
      "(4119, 1)\n"
     ]
    }
   ],
   "source": [
    "X = df[[column for column in df.columns if column != 'y']]\n",
    "y = df[['y']]\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a simple Logistic Regression Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "         no       0.92      0.98      0.95      1223\n",
      "        yes       0.59      0.23      0.34       137\n",
      "\n",
      "avg / total       0.89      0.91      0.89      1360\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression()\n",
    "clf.fit(X_train, np.ravel(y_train))\n",
    "y_pred = clf.predict(X_test)\n",
    "report = sklearn.metrics.classification_report(y_test, y_pred)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A very good performance is achieved with a simple classifier (precision of 0.89) as seen above.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a Support Vector Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "         no       0.91      0.98      0.95      1223\n",
      "        yes       0.49      0.15      0.22       137\n",
      "\n",
      "avg / total       0.87      0.90      0.87      1360\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = SVC()\n",
    "clf.fit(X_train, np.ravel(y_train))\n",
    "y_pred = clf.predict(X_test)\n",
    "report = sklearn.metrics.classification_report(y_test, y_pred)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SVM seems to perform poorly compared to the logistic regression (which is not very common)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a Ensemble classifier with logistic regression and SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "         no       0.91      0.99      0.95      1223\n",
      "        yes       0.61      0.12      0.21       137\n",
      "\n",
      "avg / total       0.88      0.90      0.87      1360\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         no       0.92      0.99      0.95      1223\n",
      "        yes       0.62      0.18      0.28       137\n",
      "\n",
      "avg / total       0.89      0.91      0.88      1360\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf1 = LogisticRegression()\n",
    "clf2 = SVC(probability=True)\n",
    "\n",
    "eclf1 = VotingClassifier(estimators=[('logist', clf1), ('svc', clf2)], voting='hard')\n",
    "eclf1 = eclf1.fit(X_train, np.ravel(y_train))\n",
    "y_pred1 = eclf1.predict(X_test)\n",
    "report1 = sklearn.metrics.classification_report(y_test, y_pred1)\n",
    "print(report1)\n",
    "\n",
    "eclf2 = VotingClassifier(estimators=[('logist', clf1), ('svc', clf2)], voting='soft')\n",
    "eclf2 = eclf2.fit(X_train, np.ravel(y_train))\n",
    "y_pred2 = eclf2.predict(X_test)\n",
    "report2 = sklearn.metrics.classification_report(y_test, y_pred2)\n",
    "print(report2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be seen that the precision for the 'Yes' class is more (both in case of hard voting and soft voting) for the ensemble classifier when compared to the Losgistic and SVC. This means this ensemble classifier has lesser false positives, which would in turn mean lesser number of campaign phone calls to people who will not likely enroll in the term deposit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding more cooks hoping for a better broth "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us add more classifiers for the voting, as we see that ensembles can be promising. \n",
    "#### Ensemble of SVC, Logistic Regression, Gaussian Naive Bayes, Random Forest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "         no       0.93      0.96      0.95      1223\n",
      "        yes       0.51      0.35      0.41       137\n",
      "\n",
      "avg / total       0.89      0.90      0.89      1360\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf1 = LogisticRegression()\n",
    "clf2 = SVC(probability=True)\n",
    "clf3 = RandomForestClassifier(random_state=1)\n",
    "clf4 = GaussianNB()\n",
    "\n",
    "eclf = VotingClassifier(estimators=[('logist', clf1), ('svc', clf2), ('rf', clf3), ('nb', clf4)], voting='soft')\n",
    "eclf = eclf.fit(X_train, np.ravel(y_train))\n",
    "y_pred = eclf.predict(X_test)\n",
    "report = sklearn.metrics.classification_report(y_test, y_pred)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this ensemble classifier we have got the best recall so far for the 'Yes' class, which means we detect more people who will enroll in the term deposit. The choice of classifier depends on what the bank wants. If the bank wants to find more people who will enroll and does not mind a few extra calls, this last ensemble can be used. But if making more calls are expensive, the previous classifier can be used."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
